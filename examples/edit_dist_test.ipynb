{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 14:24:35,987 : ***** Transfer task : SST Binary classification *****\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running new eval\n",
      "running new eval\n",
      "running new eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 14:25:10,778 : Found 16422 words with word vectors, out of         17561 words\n",
      "2018-10-17 14:25:10,844 : Computing embedding for test\n",
      "2018-10-17 14:25:11,652 : Computed test embeddings\n",
      "2018-10-17 14:25:11,654 : Computing embedding for train\n",
      "2018-10-17 14:25:16,028 : Computed train embeddings\n",
      "2018-10-17 14:25:16,030 : Computing embedding for dev\n",
      "2018-10-17 14:25:16,211 : Computed dev embeddings\n",
      "2018-10-17 14:25:16,213 : Training sklearn-LogReg with standard validation..\n",
      "/Library/Python/2.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2018-10-17 14:25:47,550 : [(u'reg:0.25', 79.47), (u'reg:0.5', 79.59), (u'reg:1', 79.59), (u'reg:2', 79.47), (u'reg:4', 79.47), (u'reg:8', 79.47)]\n",
      "2018-10-17 14:25:47,552 : Validation : best param found is reg = 0.5 with score             79.59\n",
      "2018-10-17 14:25:47,553 : Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devacc: 79.59 acc: 81.88 ndev: 67349 ntest: 1821\n",
      "0  sentences done\n",
      "100  sentences done\n",
      "200  sentences done\n",
      "300  sentences done\n",
      "400  sentences done\n",
      "500  sentences done\n",
      "600  sentences done\n",
      "700  sentences done\n",
      "800  sentences done\n",
      "900  sentences done\n",
      "1000  sentences done\n",
      "1100  sentences done\n",
      "1200  sentences done\n",
      "1300  sentences done\n",
      "1400  sentences done\n",
      "1500  sentences done\n",
      "1600  sentences done\n",
      "1700  sentences done\n",
      "1800  sentences done\n",
      "adv_embed length: 1821   1821\n",
      "0  sentences evaluated\n",
      "100  sentences evaluated\n",
      "200  sentences evaluated\n",
      "300  sentences evaluated\n",
      "400  sentences evaluated\n",
      "500  sentences evaluated\n",
      "600  sentences evaluated\n",
      "700  sentences evaluated\n",
      "800  sentences evaluated\n",
      "900  sentences evaluated\n",
      "1000  sentences evaluated\n",
      "1100  sentences evaluated\n",
      "1200  sentences evaluated\n",
      "1300  sentences evaluated\n",
      "1400  sentences evaluated\n",
      "1500  sentences evaluated\n",
      "1600  sentences evaluated\n",
      "1700  sentences evaluated\n",
      "1800  sentences evaluated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 14:26:25,790 : \n",
      "Dev acc : 79.59 Test acc : 81.88 for             SST Binary classification\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non equal count: 684\n",
      "non equal count: 792\n",
      "adversaries size: 133312\n",
      "added adv results to pass back\n"
     ]
    }
   ],
   "source": [
    "%run -i bow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'SST2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_results = results['SST2']['adv_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'adv_preds',\n",
       " u'wrong_adversaries',\n",
       " u'total_adversaries',\n",
       " u'adv_test_y',\n",
       " u'adv_test_x',\n",
       " u'uneq_adversaries',\n",
       " u'orig_predictions',\n",
       " u'model',\n",
       " u'test_y',\n",
       " u'test_x']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneq_df = pd.DataFrame(adv_results['uneq_adversaries'])\n",
    "total_df = pd.DataFrame(adv_results['total_adversaries'])\n",
    "wrong_df = pd.DataFrame(adv_results['wrong_adversaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_uneq = uneq_df/total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    311\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_uneq[prop_uneq > 0.1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1821\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_uneq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    684\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_uneq[prop_uneq > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    792\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df[wrong_df>0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    133312\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_df = total_df**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13025568\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_df.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
